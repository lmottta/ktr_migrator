"""
Pipeline: {{ pipeline_name }}
Gerado automaticamente do KTR: {{ source_ktr }}
Data: {{ generation_date }}

Este arquivo foi gerado pelo KTR Migrator
Autor: Engenheiro de Dados
"""

import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
from loguru import logger
from typing import Dict, Any, Optional
from datetime import datetime
import os
{% for import_line in custom_imports %}
{{ import_line }}
{% endfor %}

class {{ pipeline_class_name }}:
    """
    {{ pipeline_description }}
    
    Pipeline ETL gerado automaticamente a partir do KTR: {{ source_ktr }}
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Inicializa o pipeline com configura√ß√µes"""
        self.config = config or {}
        self.connections = {}
        self.metrics = {
            "records_processed": 0,
            "execution_time": 0,
            "errors": 0
        }
        self.setup_logging()
        self.setup_connections()
    
    def setup_logging(self):
        """Configura logging estruturado"""
        log_level = self.config.get("log_level", "INFO")
        log_file = self.config.get("log_file", f"logs/{{ pipeline_name.lower() }}_{datetime.now().strftime('%Y%m%d')}.log")
        
        logger.add(
            log_file,
            rotation="1 day",
            retention="30 days",
            format="{time} | {level} | {message}",
            level=log_level
        )
        
        logger.info(f"üöÄ Iniciando pipeline: {{ pipeline_name }}")
    
    def setup_connections(self):
        """Configura conex√µes com bancos de dados"""
        {% for connection in connections %}
        # Conex√£o: {{ connection.name }}
        {{ connection.name.lower() }}_url = "{{ connection.to_sqlalchemy_url() }}"
        self.connections["{{ connection.name }}"] = create_engine({{ connection.name.lower() }}_url)
        logger.info(f"üì° Conex√£o configurada: {{ connection.name }} ({{ connection.type }})")
        {% endfor %}
    
    def extract_data(self) -> pd.DataFrame:
        """
        Extra√ß√£o de dados - Stage 1
        """
        logger.info("üîÑ Iniciando extra√ß√£o de dados...")
        start_time = datetime.now()
        
        try:
            {% for extractor in extractors %}
            # {{ extractor.description }}
            {{ extractor.generate_code | indent(12) }}
            {% endfor %}
            
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ Extra√ß√£o conclu√≠da em {execution_time:.2f}s - {len(df)} registros")
            self.metrics["records_processed"] = len(df)
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o: {e}")
            self.metrics["errors"] += 1
            raise
    
    def transform_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Transforma√ß√£o de dados - Stage 2
        """
        logger.info("üîÑ Aplicando transforma√ß√µes...")
        start_time = datetime.now()
        
        try:
            {% for transformer in transformers %}
            # {{ transformer.description }}
            {{ transformer.generate_code | indent(12) }}
            {% endfor %}
            
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ Transforma√ß√µes aplicadas em {execution_time:.2f}s")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erro na transforma√ß√£o: {e}")
            self.metrics["errors"] += 1
            raise
    
    def load_data(self, df: pd.DataFrame) -> None:
        """
        Carga de dados - Stage 3
        """
        logger.info("üîÑ Carregando dados...")
        start_time = datetime.now()
        
        try:
            {% for loader in loaders %}
            # {{ loader.description }}
            {{ loader.generate_code | indent(12) }}
            {% endfor %}
            
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ Carga conclu√≠da em {execution_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Erro na carga: {e}")
            self.metrics["errors"] += 1
            raise
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """
        Valida√ß√µes de qualidade de dados
        """
        logger.info("üîç Validando qualidade dos dados...")
        
        # Valida√ß√µes b√°sicas
        if df.empty:
            logger.warning("‚ö†Ô∏è DataFrame vazio")
            return False
        
        # Verificar campos obrigat√≥rios
        required_fields = {{ required_fields | default([]) }}
        for field in required_fields:
            if field not in df.columns:
                logger.error(f"‚ùå Campo obrigat√≥rio ausente: {field}")
                return False
            
            if df[field].isnull().any():
                null_count = df[field].isnull().sum()
                logger.warning(f"‚ö†Ô∏è Campo {field} tem {null_count} valores nulos")
        
        # Verificar duplicatas
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            logger.warning(f"‚ö†Ô∏è {duplicates} registros duplicados encontrados")
        
        logger.info("‚úÖ Valida√ß√£o de dados conclu√≠da")
        return True
    
    def run_pipeline(self) -> Dict[str, Any]:
        """
        Executa pipeline completo
        """
        pipeline_start = datetime.now()
        logger.info("üéØ Executando pipeline {{ pipeline_name }}")
        
        try:
            # Extract
            df = self.extract_data()
            
            # Validate
            if not self.validate_data(df):
                raise ValueError("Falha na valida√ß√£o de dados")
            
            # Transform
            df_transformed = self.transform_data(df)
            
            # Load
            self.load_data(df_transformed)
            
            # Calcular m√©tricas finais
            pipeline_end = datetime.now()
            total_time = (pipeline_end - pipeline_start).total_seconds()
            
            self.metrics.update({
                "status": "success",
                "total_execution_time": total_time,
                "start_time": pipeline_start.isoformat(),
                "end_time": pipeline_end.isoformat(),
                "records_processed": len(df_transformed)
            })
            
            logger.info(f"üéâ Pipeline conclu√≠do com sucesso: {self.metrics}")
            return self.metrics
            
        except Exception as e:
            pipeline_end = datetime.now()
            total_time = (pipeline_end - pipeline_start).total_seconds()
            
            self.metrics.update({
                "status": "failed",
                "error": str(e),
                "total_execution_time": total_time,
                "end_time": pipeline_end.isoformat()
            })
            
            logger.error(f"üí• Pipeline falhou: {self.metrics}")
            return self.metrics
            
        finally:
            # Cleanup das conex√µes
            for name, engine in self.connections.items():
                engine.dispose()
                logger.debug(f"üîå Conex√£o fechada: {name}")

if __name__ == "__main__":
    # Exemplo de execu√ß√£o
    config = {
        "log_level": "INFO",
        "environment": "development"
    }
    
    pipeline = {{ pipeline_class_name }}(config)
    result = pipeline.run_pipeline()
    
    if result["status"] == "success":
        print(f"‚úÖ Pipeline executado com sucesso! Processados {result['records_processed']} registros")
    else:
        print(f"‚ùå Pipeline falhou: {result.get('error', 'Erro desconhecido')}")
        exit(1) 